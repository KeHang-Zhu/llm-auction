# Learning from Synthetic Laboratory: Language Models as Auction Participants

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

## Table of Contents

1. [Overview](#overview)
2. [GettingÂ Started](#getting-started)
3. [AuctionÂ Parameters](#auction-parameters)
4. [ReproducingÂ OurÂ Results](#reproducing-our-results)

---

## Overview

This repository accompanies our paperÂ â†’Â [https://openreview.net/forum?id=XZ71GHf8aB](https://openreview.net/forum?id=XZ71GHf8aB).

<p align="center">
  <img src="overview.png" alt="Project overview diagram" width="650">
</p>

If you build on this work, please cite us:

```bibtex
@article{zhuevidence,
  title  = {Evidence from the Synthetic Laboratory: Language Models as Auction Participants},
  author = {Zhu, Kehang and Shah, Anand V and Jiang, Yanchen and Horton, John Joseph and Parkes, David C}
}
```

---

## Gettingâ€¯Started

Our code supports **PythonÂ 3.9â€¯â€“â€¯3.11**.

```bash
# 1Â Â Create and activate a virtualâ€‘env
python -m venv venv
source venv/bin/activate

# 2Â Â Install dependencies
pip install edsl

# 3Â Â Configure your OpenAI key
cat > .env <<'EOF'
OPENAI_API_KEY=YOUR_KEY_HERE
EOF
```

---

## AuctionÂ Parameters

All experiments are launched via one of the three driver scripts below. Use the flags shown to configure auction mechanics.

| Auction type           | Driver script          | Required flags                                                                                           | Optional flags                                                                               |
| ---------------------- | ---------------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| **Sealedâ€‘bid**         | `main.py`              | `--seal_clock seal`                                                                                      | `--price_order {first,second,third,allpay}`<br>`--private_value {private,affiliated,common}` |
| **Clock**              | `main.py`              | `--seal_clock clock`<br>`--ascend_descend descend`                                                       | `--open_blind {open,blind}`<br>`--private_value {private,affiliated,common}`                 |
| **Ebay proxy**         | `main_ebay.py`         | `--seal_clock ebay` *(fixed)*<br>`--price_order second` *(fixed)*<br>`--private_value private` *(fixed)* | `--turns 10`<br>`--closing {true,false}`<br>`--reserve_price 60`                             |
| **Intervention study** | `main_intervention.py` | *(inherits flags from sealedâ€‘bid)*                                                                       |                                                                                              |

### Quick examples

Run a secondâ€‘price sealedâ€‘bid auction:

```bash
python main.py 
```

Run a secondâ€‘price sealedâ€‘bid auction with intervention:

```bash
python main_intervention.py 
```

Run the Ebayâ€‘style proxy auction:

```bash
python main_ebay.py
```
You can vary all the hyperparameters in these main functions.
---

## ReproducingÂ OurÂ Results

We use **[EDSL](https://docs.expectedparrot.com/en/latest/)**, whose universal remote cache stores every completed LLM call. Reâ€‘running our code with the *same prompt* and *same random seed* therefore incurs **no additional API cost**â€”results are retrieved automatically.

### 1Â Â Seeds and model settings

| Value regime        | Seeds used      |
| ------------------- | --------------- |
| Private             | **1299â€¯â€“â€¯1309** |
| Affiliated &Â Common | **1399â€¯â€“â€¯1409** |

All experiments use **GPTâ€‘4**, `temperatureâ€¯=â€¯0.5`.

### 2Â Â Cached runs (default)

```python
results = survey.by(model).run(
    remote_inference_description="cache reuse", 
    remote_inference_visibility="public"         
)
```

The snippet above will *first look in the cache*; if a match is found, the result is loaded instantly.

### 3Â Â Forcing a fresh run (optional)

To ignore the cacheâ€”for instance, when testing a new promptâ€”add `fresh=True`:

```python
results = survey.by(model).run(
    remote_inference_description="fresh run",
    remote_inference_visibility="public",
    fresh=True
)
```

### 4Â Â Verifying cache hits

EDSL prints the **JobÂ UUID** and whether it was served from cache. You can also inspect the universal cache via the web UI linked in the EDSL docs.

---

Happy experimenting! If anything is unclear, please open an issue ðŸ™Œ



## ðŸ”§ Dependencies
The main third-party package requirement are `openai` and `edsl`.

## ðŸ’¡ Contributing, Feature Asks, and Bugs
Interested collaborating in LLM as auction participants? Found a nasty bug that you would like us to squash? Please send us an email at kehangzhu@gmail.com.
